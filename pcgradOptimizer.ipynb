{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PcGrad Implementation using Python\n",
    "Pc-Grad is an optimized gradient descent algorithm which is mainly used for multitask-training."
   ],
   "id": "5b897535d50ec009"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:01:23.807071Z",
     "start_time": "2025-08-25T15:01:23.803878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from fontTools.svgLib.path import shapes\n",
    "from torch.optim.adamw import adamw"
   ],
   "id": "83f190e11654d957",
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-25T13:47:58.146459Z",
     "start_time": "2025-08-25T13:47:58.143808Z"
    }
   },
   "source": [
    "class PCGrad():\n",
    "    def __init__(self, optimizer, reduction='mean'):\n",
    "        self.optimizer = optimizer\n",
    "        return"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use a seed for replication and assign random values for input and expected results for testing. Further specifiy a fixed\n",
    "tensor assuming it's a prediction."
   ],
   "id": "c08ea8b072385c9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:29:47.138939Z",
     "start_time": "2025-08-25T12:29:47.134469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(4)\n",
    "input, expectations = torch.randn(2, 3), torch.randn(2, 4)\n",
    "predictions = torch.tensor([[-1.6053,  0.2325,  2.2399],\n",
    "                           [ 0.8473,  1.2006, -0.4016]])\n",
    "print(input,predictions)\n"
   ],
   "id": "2ea849e23ec7d919",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6053,  0.2325,  2.2399],\n",
      "        [ 0.8473,  1.2006, -0.4016]]) tensor([[-1.6053,  0.2325,  2.2399],\n",
      "        [ 0.8473,  1.2006, -0.4016]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Compute the losses of the current predictions from the model. First use the derivation of the MSE functions\n",
    "and then apply the loss functions on the input and predictions done by the model, this is\n",
    "our starting point for PCGrad."
   ],
   "id": "6883f63be22f42ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:29:50.303809Z",
     "start_time": "2025-08-25T12:29:50.300624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss1_fn, loss2_fn = nn.L1Loss(), nn.MSELoss()\n",
    "print(loss1_fn(input, predictions), loss2_fn(input, predictions))"
   ],
   "id": "369dbe1169de95df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8007e-05) tensor(1.0211e-09)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a random tensor which is considered as our parameters. Then initialize an optimizer.",
   "id": "40e1598280c1b468"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T13:50:23.374723Z",
     "start_time": "2025-08-25T13:50:20.335602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parameter = torch.randn(1, 3)\n",
    "optimizer = torch.optim.Adam(params=[parameter])"
   ],
   "id": "5c50b94f2b1ccf02",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we compute the projected conflicted gradient or the gradients for the give loss objectives.",
   "id": "e7029744bcfa6394"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T13:50:28.898705Z",
     "start_time": "2025-08-25T13:50:28.895601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pc_backward(objectives):\n",
    "    grads, shapes, has_grads = pc_package_grad(objectives)\n",
    "    pc_grad = pc_project_conflicting(grads, has_grads)\n",
    "    pc_Grad = pc_unflatten_grad(pc_grad, shapes[0])\n",
    "    self.pc_set_grad(pc_grad)\n",
    "    return"
   ],
   "id": "927d9263c4294367",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Specify parameters from optimizer and package them into an array of gradient values, shapes, and present bits.",
   "id": "61036eb64970490c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T13:50:30.322572Z",
     "start_time": "2025-08-25T13:50:30.319690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pc_package_grad(objectives):\n",
    "    grads, shapes, has_grads = [], [], []\n",
    "    for objective in objectives:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        objective.backward(retain_grad=True)\n",
    "        grad, shape, has_grad = pc_retrieve_grad()\n",
    "        grads.append(flatten_grads(grads, shapes))\n",
    "        has_grads.append(flatten_grads(has_grad, shape))\n",
    "        shapes.append(shape)\n",
    "    return grads, shapes, has_grads"
   ],
   "id": "1f8f4e36a4413893",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The function will get the gradient of the parameters form the given model/network with specific objective.\n",
    "It will differentiate between gradients that actually present in the given parameter tensor or not.\n"
   ],
   "id": "7052c4eccba7ddba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T12:30:29.474413Z",
     "start_time": "2025-08-25T12:30:29.471135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pc_retrieve_grad():\n",
    "    grad, shape, has_grad = [], [], []\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is None:\n",
    "                shape.append(param.shape)\n",
    "                grad.append(torch.zeros_like(param).to(param.device))\n",
    "                has_grad.append(torch.zeros_like(param).to(param.device))\n",
    "                continue\n",
    "            shape.append(param.grad.shape)\n",
    "            grad.append(param.grad.clone())\n",
    "            has_grad.append(torch.ones_like(param).to(param.device))\n",
    "    return grad, shape, has_grad\n"
   ],
   "id": "a31906a8bb3545a4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Flattening is a technique that is used to convert multi-dimensional arrays into 1-D array. In this case concat all the gradients to a 1-Dimensional Array",
   "id": "3e6faa7157e93f9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T13:56:55.331359Z",
     "start_time": "2025-08-25T13:56:55.326652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flatten_grads(grads, shapes):\n",
    "    flatten_grad = torch.cat([g.flatten() for g in grads])\n",
    "    return flatten_grad\n",
    "\n",
    "ex_grads = torch.randn(4,4)\n",
    "print(\"grads: \", ex_grads, \" shapes: \", ex_grads.shape)\n",
    "flatted_ex_grads = flatten_grads(ex_grads, ex_grads.shape)\n",
    "print(\"flatted grads: \", flatted_ex_grads)\n"
   ],
   "id": "40d0b54a4f5317c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads:  tensor([[-0.6889, -0.8556, -0.0143, -0.4549],\n",
      "        [ 1.5472, -1.5344, -0.5646, -0.2765],\n",
      "        [-1.0467, -1.1993,  0.2494, -1.2676],\n",
      "        [ 0.7497, -0.1904,  1.0569,  1.0551]])  shapes:  torch.Size([4, 4])\n",
      "flatted grads:  tensor([-0.6889, -0.8556, -0.0143, -0.4549,  1.5472, -1.5344, -0.5646, -0.2765,\n",
      "        -1.0467, -1.1993,  0.2494, -1.2676,  0.7497, -0.1904,  1.0569,  1.0551])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This method will compare the projecting direction of the different gradients computed for the multiple tasks. If they do not project in the same direction\n",
    "they will be corrected and merged in the end."
   ],
   "id": "3c22740f5da89b82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:33:27.729794Z",
     "start_time": "2025-08-25T15:33:27.723637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pc_project_conflicting(grads, has_grads, reduction=\"sum\", shapes=None):\n",
    "    # Boolean mask: coordinates present ALL tasks\n",
    "    shared = torch.stack(has_grads, dim=0).all(dim=0)\n",
    "    # Work on a deepcopy so we don't mutate the caller's grads\n",
    "    pc_grad = [g.clone() for g in grads]\n",
    "    num_task = len(has_grads)\n",
    "    # For each task i, project away components that conflict with other\n",
    "    # tasks' gradients\n",
    "    for i in range (num_task):\n",
    "        order = list(range(num_task))\n",
    "        print(\"order \", order)\n",
    "        random.shuffle(order)\n",
    "        for j in order:\n",
    "            if i == j:\n",
    "                continue # Edge case handling\n",
    "            gi, gj =  pc_grad[i], grads[j]\n",
    "            print(\"gi, gj: \", gi, gj)\n",
    "            dot = torch.dot(gi, gj) # provides the direction\n",
    "            print(\"dot product: \", dot)\n",
    "            if dot < 0: # gradients are pointing in different directions\n",
    "                denorm = gj.norm().pow(2) # simple algebra solves for y and denormalize and makes positive\n",
    "                print(\"denorm: \", denorm)\n",
    "                if denorm > 0:\n",
    "                    # Correct projection, subtract component of gi along gj\n",
    "                    pc_grad[i] = gi - (dot/denorm) * gj\n",
    "                    gi = pc_grad[i] # saving also for gi for next iteration\n",
    "                    print(\"gi: \", gi)\n",
    "    stacked = torch.stack(pc_grad, dim=0) # concat the corrected gradient\n",
    "    if reduction == \"sum\":\n",
    "        merged = stacked.sum(dim=0)\n",
    "    elif reduction == \"mean\":\n",
    "        merged = stacked.mean(dim=0)\n",
    "    else:\n",
    "        raise Exception(\"reduction must be 'sum' or 'mean'\")\n",
    "    return merged\n"
   ],
   "id": "7cdc938ed273c561",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Expending the gradient tensor in to the desired shape.",
   "id": "d1ddb5bc9b78086"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def pc_unflatten_grad(grads, shapes):\n",
    "    unflatten_grad, idx = [], 0\n",
    "    for shape in shapes:\n",
    "        length = np.prod(shape)\n",
    "        unflatten_grad.append(grads[idx:idx+length].view(shape).clone())\n",
    "        idx += length\n",
    "    return unflatten_grad\n",
    "\n",
    "\n"
   ],
   "id": "9cb0552528b1d3ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sets the corrected gradients back again to the desired parameters from the orginal\n",
    "optimizer."
   ],
   "id": "a1669381bd11d927"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T15:42:59.088044Z",
     "start_time": "2025-08-25T15:42:59.084941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pc_set_grad(self, grads):\n",
    "    idx = 0\n",
    "    for group in self.optimizer.para_groups:\n",
    "        for p in group['params']:\n",
    "            p.grad = grads[idx]\n",
    "            idx += 1\n",
    "    return\n"
   ],
   "id": "32f9f7b80f1f9ef0",
   "outputs": [],
   "execution_count": 57
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
